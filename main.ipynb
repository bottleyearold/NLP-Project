{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/00/86znvyf90q3d6rpvdrxpcz0h0000gn/T/ipykernel_82353/1131140429.py:38: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use(\"seaborn-whitegrid\")\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/dor.frechter/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/dor.frechter/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Libraries for general purpose\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Text cleaning\n",
    "import re\n",
    "import string\n",
    "import emoji\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Data preprocessing\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from langdetect import detect, LangDetectException\n",
    "import contractions\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "# Set seed for reproducibility\n",
    "import random\n",
    "\n",
    "import torch\n",
    "seed_value = 2042\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "# Set style for plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.despine()\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "plt.rc(\"figure\", autolayout=True)\n",
    "plt.rc(\"axes\", labelweight=\"bold\", labelsize=\"large\", titleweight=\"bold\", titlepad=10)\n",
    "\n",
    "# Define stop words for text cleaning\n",
    "seed_value = 2042\n",
    "random.seed(seed_value)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "# Initialize lemmatizer for text cleaning\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 47656 entries, 0 to 47691\n",
      "Data columns (total 2 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   tweet_text          47656 non-null  object\n",
      " 1   cyberbullying_type  47656 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('cyberbullying_tweets.csv')\n",
    "df = df[~df.duplicated()]\n",
    "df.head()\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cyberbullying_type\n",
       "religion               7997\n",
       "age                    7992\n",
       "ethnicity              7959\n",
       "gender                 7948\n",
       "not_cyberbullying      7937\n",
       "other_cyberbullying    7823\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cyberbullying_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter special characters such as & and $ present in some words\n",
    "def filter_chars(text):\n",
    "    return ' '.join('' if ('$' in word) or ('&' in word) else word for word in text.split())\n",
    "\n",
    "# Remove multiple spaces\n",
    "def remove_mult_spaces(text):\n",
    "    return re.sub(r\"\\s\\s+\", \" \", text)\n",
    "\n",
    "# Function to check if the text is in English, and return an empty string if it's not\n",
    "def filter_non_english(text):\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "    except LangDetectException:\n",
    "        lang = \"unknown\"\n",
    "    return text if lang == \"en\" else \"\"\n",
    "\n",
    "# Remove punctuations, stopwords, links, mentions and new line characters\n",
    "def strip_all_entities(text):\n",
    "    text = re.sub(r'\\r|\\n', ' ', text.lower())  # Replace newline and carriage return with space, and convert to lowercase\n",
    "    text = re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", text)  # Remove links and mentions\n",
    "    text = re.sub(r'[^\\x00-\\x7f]', '', text)  # Remove non-ASCII characters\n",
    "    banned_list = string.punctuation\n",
    "    table = str.maketrans('', '', banned_list)\n",
    "    text = text.translate(table)\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "    return text\n",
    "\n",
    "# Clean hashtags at the end of the sentence, and keep those in the middle of the sentence by removing just the # symbol\n",
    "def clean_hashtags(tweet):\n",
    "    # Remove hashtags at the end of the sentence\n",
    "    new_tweet = re.sub(r'(\\s+#[\\w-]+)+\\s*$', '', tweet).strip()\n",
    "    \n",
    "    # Remove the # symbol from hashtags in the middle of the sentence\n",
    "    new_tweet = re.sub(r'#([\\w-]+)', r'\\1', new_tweet).strip()\n",
    "    \n",
    "    return new_tweet\n",
    "\n",
    "\n",
    "# Remove repeated punctuation\n",
    "def remove_repeated_punctuation(text):\n",
    "    return re.sub(r'[\\?\\.\\!]+(?=[\\?\\.\\!])', '', text)\n",
    "\n",
    "# Remove extra whitespace\n",
    "def remove_extra_whitespace(text):\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "\n",
    "def remove_url_shorteners(text):\n",
    "    return re.sub(r'(?:http[s]?://)?(?:www\\.)?(?:bit\\.ly|goo\\.gl|t\\.co|tinyurl\\.com|tr\\.im|is\\.gd|cli\\.gs|u\\.nu|url\\.ie|tiny\\.cc|alturl\\.com|ow\\.ly|bit\\.do|adoro\\.to)\\S+', '', text)\n",
    "\n",
    "# Remove spaces at the beginning and end of the tweet\n",
    "def remove_spaces_tweets(tweet):\n",
    "    return tweet.strip()\n",
    "\n",
    "# Remove short tweets\n",
    "def remove_short_tweets(tweet, min_words=3):\n",
    "    words = tweet.split()\n",
    "    return tweet if len(words) >= min_words else \"\"\n",
    "\n",
    "# Lemmatize words\n",
    "def lemmatize(text):\n",
    "    words = word_tokenize(text)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# Remove short words\n",
    "def remove_short_words(text, min_len=2):\n",
    "    words = text.split()\n",
    "    long_words = [word for word in words if len(word) >= min_len]\n",
    "    return ' '.join(long_words)\n",
    "\n",
    "# Replace elongated words with their base form\n",
    "def replace_elongated_words(text):\n",
    "    regex_pattern = r'\\b(\\w+)((\\w)\\3{2,})(\\w*)\\b'\n",
    "    return re.sub(regex_pattern, r'\\1\\3\\4', text)\n",
    "\n",
    "def strip_emoji(text):\n",
    "     # Remove emojis (covers most common unicode emojis)\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "        u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "        u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "        u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "        u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "        u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "        u\"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "        u\"\\U000024C2-\\U0001F251\"  # Enclosed characters\n",
    "        \"]+\",\n",
    "        flags=re.UNICODE,\n",
    "    )\n",
    "    return emoji_pattern.sub(r\"\", text)\n",
    "\n",
    "def remove_numbers(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "def pipeline(tweet):\n",
    "    tweet = strip_emoji(tweet)\n",
    "    tweet = filter_non_english(tweet)\n",
    "    tweet = strip_all_entities(tweet)\n",
    "    tweet = clean_hashtags(tweet)\n",
    "    tweet = filter_chars(tweet)\n",
    "    tweet = remove_mult_spaces(tweet)\n",
    "    tweet = remove_numbers(tweet)\n",
    "    tweet = lemmatize(tweet)\n",
    "    tweet = remove_short_words(tweet)\n",
    "    tweet = replace_elongated_words(tweet)\n",
    "    tweet = remove_repeated_punctuation(tweet)\n",
    "    tweet = remove_extra_whitespace(tweet)\n",
    "    tweet = remove_url_shorteners(tweet)\n",
    "    tweet = remove_spaces_tweets(tweet)\n",
    "    tweet = remove_short_tweets(tweet)\n",
    "    tweet = ' '.join(tweet.split())  \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_clean'] = [pipeline(tweet) for tweet in df['tweet_text']]\n",
    "df.drop_duplicates(\"text_clean\", inplace=True)\n",
    "df.cyberbullying_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"cyberbullying_type\"]!=\"other_cyberbullying\"]\n",
    "bullying_classes = [\"religion\",\"age\",\"ethnicity\",\"gender\",\"not bullying\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
